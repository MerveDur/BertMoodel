# BertMoodel
## Project: COMPARISON OF BERT GRAPH TRANSFORMER APPLICATION WITH CLASSIC BERT MODEL/BERT ÇIZGE TRANSFORMER UYGULAMASININ KLASIK BERT MODELI ILE KARŞILAŞTIRILMASI
## Dataset: coraDataset
English
##ABSTRACT
The aim of this project is to compare the BERT graphic transformer application
with the classical BERT model. These two models were compared by making text
classification of the sentences in the appropriate dataset. BERT is based on a pretrained, open source NPL (Natural Language Processing) model. While it examines
the whole sentence to find the missing word in the search query, it differs from other
models with this feature. BERT, which also uses machine learning algorithms, has a
two-way language processing feature. It tries to understand the relationship of each
word with another word. It uses a more complex masked language model as opposed
to a superficial bidirectional language processing that goes right to left, left to right.
The classical Bert model and the Graph Bert model were used for this comparison.
The two models were compared by performing text classification. Text classification is a
machine learning technique of categorizing open-ended text into a predefined category.
Text classifiers can be used to organize, structure and categorize almost any type of text
from documents, medical studies and files, and the entire web. Text classification was
run on these two models (datasets fitted to both models) and results were compared.
BERT and TRANSFORMER: In NLP, the dominant sequence transduction models are based on complex recurrent .However, the inherently sequential nature precludes
parallelization within training examples. Graph Bert is a new network architecture, this
transformer based solely on attention mechanisms, dispensing with recurrence and
convolutions entirely. In recent years, TRANSFORMER and BERT based learning
approaches have been used extensively in various learning tasks.
While comparing these two models, train loss and accuracy values were compared
with each other. Train loss graphs are drawn.
Keywords: Bert, Graph Bert,Text Classification

Türkçe
##ÖZET
Bu projenin amacı BERT grafik tranformatör uygulamasının klasik BERT modeli
ile karşılaştırılmasıdır. Uygun verisetindeki cümlelerin metin sınıflandırması yapılarak
bu iki model karşılaştırıldı. BERT, önceden eğitilmiş, açık kaynak kodlu bir NPL(Doğal
Dil İşleme) modeline dayanır. Arama sorgusundaki eksik olan kelimeyi bulabilmek
için tüm cümleyi incelerken, bu özelliği ile de diğer modellerden ayrılmaktadır.Makine
öğrenimi algoritmalarını da kullanan BERT, iki yönlü bir dil işleme özelliğine sahiptir.
Her kelimenin diğer bir kelimeyle ilişkisini anlamaya çalışır. Sağdan sola, soldan sağa
giden yüzey- sel çift yönlü bir dil işlemesinin aksine daha karmaşık maskeli dil modeli
kullanır.
Bu karşılaştırma için klasik Bert modeli ve Graph Bert modeli kullanıldı. Metin
sınıflandırılması yapılarak iki model karşılaştırıldı.Metin sınıflandırması, açık uçlu
metne önceden tanımlanmış bir kategorilere ayırma makine öğrenimi tekniğidir . Metin
sınıflandırıcılar, belgelerden, tıbbi çalışmalardan ve dosyalardan ve tüm web’den hemen
hemen her tür metni düzenlemek, yapılandırmak ve kategorilere ayırmak için kullanılabilir.Metin sınıflandırılması bu iki model üzerinde çalıştırılarak (datasetleri iki modele
uygun hale getirildi) sonuçlar karşılaştırıldı.
Bu iki model karşılaştırılırken train loss vs accuracy değerleri birbirleriyle kıyaslandı.
Train loss grafikleri çizildi.
Anahtar Kelimeler: Bert, Graph Bert,Metin Sınıflandırma
